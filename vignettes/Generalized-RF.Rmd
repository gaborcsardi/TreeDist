---
title: "Generalized Robinson-Foulds distances"
author: "Martin R. Smith"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
csl: ../inst/apa-old-doi-prefix.csl
vignette: >
  %\VignetteIndexEntry{Generalized Robinson-Foulds distances}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r init, message=FALSE, warning=FALSE}
library('ape')
library('TreeDist')
tree1 <- read.tree(text='((A, B), ((C, (D, E)), (F, (G, (H, I)))));')
tree2 <- read.tree(text='((A, B), ((C, D, (E, I)), (F, (G, H))));')
```

This vignette will describe the similarity measures employed by the 
generalized Robinson-Foulds distances implemented in this package.

# Mutual phylogenetic information

```{r, fig.width=6 , fig.align='center'}
VisualizeMatching(MutualPhylogeneticInfo, tree1, tree2, 
                  Plot=TreeDistPlot, leaveRoom=FALSE, matchZeros = FALSE)
```

Pairs are scored according to their mutual phylogenetic information.

The phylogenetic information content of a partition is defined as
the negative logarithm of the proportion of all bifurcating _n_-tip trees that
contain that partition.  Thus a more even bipartition, which is consistent with
a smaller proportion of trees, contains more phylogenetic information.

The mutual phylogenetic information of two partitions is the information that
they contain in common; that is, the sum of the information content of the two 
splits, minus the joint information (which is the negative logarithm of the 
proportion of trees that contain both partitions).

Consider the splits `ABCDEF:GHI` and `ABCDEFI:GH`.  

```{r}
library('TreeSearch')
treesMatchingSplit <- c(
  ABCDEF.GHI = TreesMatchingSplit(6, 3),
  ABCDEFI.GH = TreesMatchingSplit(7, 2)
)
treesMatchingSplit

proportionMatchingSplit <- treesMatchingSplit / NUnrooted(9)
proportionMatchingSplit

splitInformation <- -log2(proportionMatchingSplit)
splitInformation

treesMatchingBoth <- TreesConsistentWithTwoSplits(9, 6, 7)
jointInformation <- -log2(treesMatchingBoth / NUnrooted(9))

mutualInformation <- sum(splitInformation) - jointInformation
mutualInformation

# Or more concisely:
SplitMutualInformation(n = 9, 6, 7)
```


<!--#TODO: 1 != 1; need to normalize.-->

## The problem with arboreal matchings

The Mutual Phylogenetic Information is an arboreal similarity score: that is to
say, it assigns zero similarity to partitions that cannot both occur on a 
single tree.  This leads to problematic behaviour in certain cases: for example,
when the position of two distant taxa is swapped, similarities within a tree can
be obscured.  The two trees below are identical but for the position of _A_ and
_J_, yet receive zero similarity under arboreal metrics.

```{r, fig.width=6 , fig.align='center'}
VisualizeMatching(MutualPhylogeneticInfo, matchZeros = FALSE, 
  AtoJ <- ape::read.tree(text='(((((A, B), C), D), E), (F, (G, (H, (I, J)))));'),
  AotJ <- ape::read.tree(text='(((((J, B), C), D), E), (F, (G, (H, (I, A)))));'),
                  Plot=TreeDistPlot, leaveRoom = FALSE, prune=c(5, 18))
```

# Mutual clustering information

Clustering information provides a non-arboreal information-based
tree distance metric.
As desired, this assigns a non-zero similarity to the case that received zero
similarity under an arboreal matching:

```{r}
MutualClusteringInfo(AtoJ, AotJ)
```

```{r, fig.width=6 , fig.align='center'}
VisualizeMatching(MutualClusteringInfo, AtoJ, AotJ, matchZeros = FALSE, 
                  Plot=TreeDistPlot, leaveRoom = FALSE, prune=c(5, 18))
```

Here, each partition is viewed as clustering the terminals into two groups.
Consider a pair of partitions, Y~1~ and Y~2~. 
If the partitions are identical, then if an observer knows which group a
terminal is in Y~1~, then this gives all the information they need to know
which group it is in in Y~2~.  As the partitions become more different, the
disposition of Y~1~ gives less information about the configuration of Y~2~;
the probability that a guess based on Y~1~ will be correct decreases, and 
the mutual information decreases accordingly.

The mutual clustering information has the notable property that no pair
of splits will be allocated zero similarity.  As such, even a dissimilar 
matching (such as `HI:ABCDEFG` => `EI:ABCDFGH` below) 
is preferred to leaving a partition unpaired.

```{r}
VisualizeMatching(MutualClusteringInfo, tree1, tree2, 
                  Plot=TreeDistPlot, leaveRoom=FALSE, matchZeros = FALSE)
```

# Nye _et al._

```{r}
NyeTreeSimilarity(tree1, tree2, normalize=FALSE)
```
The Nye _et al_. [-@Nye2006] tree similarity metric scores pairs by
considering the elements held in common between subsets of each partition.

Consider a pair of partitions `ABCDEF:GHIJ` and `ABCDEIJ:FGH`.
These can be aligned thus:

`ABCDEF  : GHIJ`
`ABCDE IJ:FGH  `

The first pair of subsets, `ABCDEF` and `ABCDEIJ`, have five elements in common 
(`ABCDE`), and together encompass eight elements (`ABCDEFIJ`).  Their 
_subset score_ is thus 5/8.

The second pair of subsets, `GHIJ` and `FGH`, have two elements (`GH`) in common,
of the five total (`FGHIJ`), and hence receive a subset score of 2/5.

This split alignment then receives an _alignment score_ corresponding to the 
lower of the two subset scores, 2/5.

We must now consider the other alignment of this pair of partitions,


`ABCDEF  :     GHIJ`
`     FGH:ABCDE  IJ`

This yields subset scores of 1/8 and 2/9, and thus has an alignment score of 1/8.
This alignment gives a lower score than the other, so is disregarded.  The 
pair of partitions is allocated a similarity score corresponding to the 
better alignment: 2/5.

As such, partitions that match exactly will receive a similarty score of 1,
in a manner analagous to the Robinson-Foulds distance.  (This is despite the
fact that some partitions are more likely to match than others.)
And it is not possible for a pair of partitions to receive a zero similarity 
score.

```{r}
VisualizeMatching(NyeTreeSimilarity, tree1, tree2, 
                  Plot=TreeDistPlot, leaveRoom=FALSE, matchZeros = FALSE)
```

B&ouml;cker _et al_. [-@Bocker2013] propose expressly prohibiting contradictory
pairings, i.e. pairs of partitions that could never occur in the same tree.
Such an 'arboreal' matching would preclude the pairing of `ABCFGHI:DE` with
`ABCDFGH:EI`:

```{r}
JaccardRobinsonFoulds(tree1, tree2, similarity = TRUE)

VisualizeMatching(JaccardRobinsonFoulds, tree1, tree2,
                  Plot=TreeDistPlot, leaveRoom=FALSE, matchZeros = FALSE)
```

B&ouml;cker _et al_. [-@Bocker2013] also suggest raising the partition 
similarity score defined above to an 
arbitrary exponent; as the exponent grows towards infinity, the metric
converges to the Robinson-Foulds metric.

```{r}
JaccardRobinsonFoulds(tree1, tree2, k = 2)
JaccardRobinsonFoulds(tree1, tree2, k = 5)
JaccardRobinsonFoulds(tree1, tree2, k = Inf)
RobinsonFoulds(tree1, tree2)
```

# Matching Split Distance

Bogdanowicz & Giaro [@-Bogdanowicz2012] propose an alternative distance, which
they term the Matching Split Distance.

```{r}
MatchingSplitDistance(tree1, tree2)
VisualizeMatching(MatchingSplitDistance, tree1, tree2)
```

Note that the visualization shows the difference, rather than the similarity,
between splits.

Similar to the Nye _et al_. similarity metric, this method compares the subsets
implied by a pair of partitions.  Here, the relevanat quantity is the number
of elements that must be moved from one subset to another in order to make
the two partitions identical.  With the pair of partitions

`ABCDEF  : GHIJ`
`ABCDE IJ:FGH  `

three elements (_F_, _I_ and _J_) must be moved before the partitions are 
identical; as such, the pair of partitions are assigned a difference score 
(not a similarity score) of three.

```{r}
MatchingSplitDistance(read.tree(text='((a, b, c, d, e, f), (g, h, i, j));'),
                      read.tree(text='((a, b, c, d, e, i, j), (g, h, f));'))
```

This distance is difficult to normalize, as its maximum value is difficult
to calculate.

## Information theoretic alternative



# References